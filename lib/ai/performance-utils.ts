/**
 * AI Performance Optimization Utilities
 * Generated by performance optimization script
 */

export interface AIPerformanceMetrics {
  predictionTime: number
  accuracy: number
  cacheHitRate: number
  batchEfficiency: number
}

export class AIPerformanceMonitor {
  private metrics: AIPerformanceMetrics[] = []
  private cache = new Map<string, { result: any; timestamp: number; confidence: number }>()
  private readonly cacheTimeout = 3600000 // 1 hour
  private readonly confidenceThreshold = 0.7

  async trackPrediction<T>(
    operation: () => Promise<{ result: T; confidence: number }>,
    cacheKey?: string
  ): Promise<{ result: T; confidence: number; fromCache: boolean }> {
    const startTime = performance.now()
    
    // Check cache first
    if (cacheKey && this.cache.has(cacheKey)) {
      const cached = this.cache.get(cacheKey)!
      if (Date.now() - cached.timestamp < this.cacheTimeout) {
        const endTime = performance.now()
        this.recordMetrics({
          predictionTime: endTime - startTime,
          accuracy: cached.confidence,
          cacheHitRate: 1,
          batchEfficiency: 1
        })
        
        return {
          result: cached.result,
          confidence: cached.confidence,
          fromCache: true
        }
      } else {
        this.cache.delete(cacheKey)
      }
    }
    
    // Execute prediction
    const prediction = await operation()
    const endTime = performance.now()
    
    // Cache result if confidence is high enough
    if (cacheKey && prediction.confidence >= this.confidenceThreshold) {
      this.cache.set(cacheKey, {
        result: prediction.result,
        timestamp: Date.now(),
        confidence: prediction.confidence
      })
    }
    
    // Record metrics
    this.recordMetrics({
      predictionTime: endTime - startTime,
      accuracy: prediction.confidence,
      cacheHitRate: 0,
      batchEfficiency: 1
    })
    
    return {
      ...prediction,
      fromCache: false
    }
  }

  private recordMetrics(metrics: AIPerformanceMetrics) {
    this.metrics.push(metrics)
    
    // Keep only last 1000 metrics
    if (this.metrics.length > 1000) {
      this.metrics = this.metrics.slice(-1000)
    }
  }

  getPerformanceStats() {
    if (this.metrics.length === 0) {
      return null
    }
    
    const avgPredictionTime = this.metrics.reduce((sum, m) => sum + m.predictionTime, 0) / this.metrics.length
    const avgAccuracy = this.metrics.reduce((sum, m) => sum + m.accuracy, 0) / this.metrics.length
    const cacheHitRate = this.metrics.reduce((sum, m) => sum + m.cacheHitRate, 0) / this.metrics.length
    
    return {
      averagePredictionTime: Math.round(avgPredictionTime),
      averageAccuracy: Math.round(avgAccuracy * 100) / 100,
      cacheHitRate: Math.round(cacheHitRate * 100) / 100,
      totalPredictions: this.metrics.length,
      cacheSize: this.cache.size
    }
  }

  clearCache() {
    this.cache.clear()
  }
}

// Global instance
export const aiPerformanceMonitor = new AIPerformanceMonitor()

// Batch processing utility
export class AIBatchProcessor<T, R> {
  private queue: Array<{ input: T; resolve: (result: R) => void; reject: (error: Error) => void }> = []
  private processing = false
  private readonly batchSize = 10
  private readonly timeout = 5000

  async process(input: T, processor: (batch: T[]) => Promise<R[]>): Promise<R> {
    return new Promise((resolve, reject) => {
      this.queue.push({ input, resolve, reject })
      
      if (!this.processing) {
        this.processBatch(processor)
      }
    })
  }

  private async processBatch(processor: (batch: T[]) => Promise<R[]>) {
    this.processing = true
    
    try {
      while (this.queue.length > 0) {
        const batch = this.queue.splice(0, this.batchSize)
        const inputs = batch.map(item => item.input)
        
        try {
          const results = await Promise.race([
            processor(inputs),
            new Promise<R[]>((_, reject) => 
              setTimeout(() => reject(new Error('Batch processing timeout')), this.timeout)
            )
          ])
          
          batch.forEach((item, index) => {
            if (results[index] !== undefined) {
              item.resolve(results[index])
            } else {
              item.reject(new Error('No result for batch item'))
            }
          })
          
        } catch (error) {
          batch.forEach(item => item.reject(error as Error))
        }
      }
    } finally {
      this.processing = false
    }
  }
}
